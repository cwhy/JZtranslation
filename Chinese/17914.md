![ ](../../UserFiles/Image/activity/darwin.jpg)

**时间：5****月6号下午两点半 **

**地点：**海淀区三号会所（地址路线见 <http://www.douban.com/photos/album/13343561/>） 

**费用：**参加活动每人15元(含一份饮料) [注：这是三号会所收取的场地费，我们的活动是免费的]

**主讲人**： 王钺 

**讲稿**：<http://www.kuaipan.com.cn/file/id_17495416136272783.htm>

**多贝在线教室**：<http://www.duobei.com/room/3044611067>

统计学习读书会已经进行了五期，其中我们一起学习了很多内容

  * 高斯假设下的分类与降维（QDA, LDA）
  * Logistic Regression
  * Sparsity 
  * Random Forest

过程中我们也讨论了很多有趣的问题，例如：

  * 为什么Logistic Regression中使用Sigmoid transform？
  * 为什么在回归或者分类问题中进行变量选择？
  * 为什么添加了L1约束的线性回归通常会有更好的效果？
  * 面对不同的问题和不同数量的数据，我们如何在工具箱中选择最合适的方法？

这些问题我们已经在一些视角下得到了部分的回答，

不过我相信在第六次读书会之后我们对于这些问题会有新的更深更广泛的理解

【活动内容】

在第六期活动中，清华大学老师王钺将给我们介绍有限样本下的统计学习理论。

内容包括：

统计学习的目标 -- 最小化实际风险

最小化实际风险的方法 -- 最小化经验风险和结构风险

统计学习理论的应用 -- SVM

  

  

